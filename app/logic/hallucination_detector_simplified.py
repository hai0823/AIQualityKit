#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹»è§‰æ£€æµ‹å™¨ - ç®€åŒ–ç‰ˆï¼Œé€‚é…ç°æœ‰é¡¹ç›®æ¶æ„
æ£€æµ‹AIç”Ÿæˆå†…å®¹ä¸­çš„å¹»è§‰é—®é¢˜ï¼Œè¿›è¡Œäº”åˆ†ç±»åˆ†æ
"""

import pandas as pd
import json
import asyncio
import aiohttp
from typing import Dict, Any, List, Optional
import logging
from ..utils.api_client import create_api_client

logger = logging.getLogger(__name__)

class HallucinationDetector:
    def __init__(self, provider: str = "deepseek", api_key: str = None, 
                 base_url: str = None, model: str = None, concurrent_limit: int = 10):
        """
        åˆå§‹åŒ–å¹»è§‰æ£€æµ‹å™¨
        
        Args:
            provider: APIæä¾›å•† ('alibaba', 'openai', 'deepseek')
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URLï¼ˆå¯é€‰ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼‰
            concurrent_limit: å¹¶å‘é™åˆ¶
        """
        self.provider = provider.lower()
        self.concurrent_limit = concurrent_limit
        
        # ä½¿ç”¨ç°æœ‰çš„APIå®¢æˆ·ç«¯
        self.api_client = create_api_client(provider, api_key, base_url, model)
        self.api_key = self.api_client.api_key
        self.base_url = self.api_client.base_url
        self.model = self.api_client.model
        
        if not self.api_key:
            raise ValueError(f"APIå¯†é’¥æœªè®¾ç½®ï¼Œè¯·æä¾›{provider} APIå¯†é’¥")
        
        print(f"æ­£åœ¨ä½¿ç”¨æä¾›å•†: {self.provider}")
        print(f"æ­£åœ¨ä½¿ç”¨æ¨¡å‹: {self.model}")
        print(f"å¹¶å‘é™åˆ¶: {self.concurrent_limit}")

    def create_hallucination_prompt(self, question: str, answer: str, combined_citations: str) -> str:
        """åˆ›å»ºå¹»è§‰æ£€æµ‹çš„æç¤ºè¯ï¼Œä½¿ç”¨ä¸åŸç‰ˆç›¸åŒçš„æ ¼å¼"""
        system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªå¤§æ¨¡å‹è¾“å‡ºæ£€æµ‹ä¸“å®¶ã€‚"
            "ä»»åŠ¡ï¼šç»™å®šä¸€ä¸ªå¤§æ¨¡å‹çš„å›ç­”ï¼ˆç­”æ¡ˆï¼‰å’Œå®ƒä½¿ç”¨çš„å¼•ç”¨æ–‡ç« ï¼ˆå¼•æ–‡ï¼‰ï¼Œ"
            "åˆ¤æ–­ç­”æ¡ˆæ˜¯å¦åŸºäºå¼•æ–‡ï¼Œæ˜¯å¦å­˜åœ¨å¹»è§‰ï¼Œå¹¶ç»™å‡ºå…·ä½“çš„å¹»è§‰ç±»å‹åˆ†ç±»ã€‚"
            "\n\nå¹»è§‰ç±»å‹åˆ†ç±»æ ‡å‡†ï¼š"
            "\n1. æ— å¹»è§‰ï¼šç­”æ¡ˆä¸å¼•æ–‡å†…å®¹å®Œå…¨ä¸€è‡´æˆ–é«˜åº¦ä¸€è‡´ï¼Œæ— çŸ›ç›¾ã€æ— è™šæ„ã€‚"
            "\n2. äº‹å®å†²çªï¼šç­”æ¡ˆå’Œå¼•æ–‡åœ¨åŒä¸€äº‹å®é—®é¢˜ä¸Šä¿¡æ¯ç›¸äº’çŸ›ç›¾ã€‚"
            "\n3. æ— ä¸­ç”Ÿæœ‰ï¼šç­”æ¡ˆåŒ…å«å¼•æ–‡ä¸­å®Œå…¨æ²¡æœ‰å‡ºç°çš„è™šæ„ä¿¡æ¯ã€‚"
            "\n4. æŒ‡ä»¤è¯¯è§£ï¼šç­”æ¡ˆæ•´ä½“ä¸»é¢˜æˆ–æ–¹å‘åç¦»ï¼Œå¼•æ–‡ä¸ç­”æ¡ˆä¸åœ¨ä¸€ä¸ªè¯­å¢ƒä¸‹ã€‚"
            "\n5. é€»è¾‘é”™è¯¯ï¼šç­”æ¡ˆçš„æ¨ç†é“¾æ¡æˆ–é€»è¾‘å…³ç³»æœ‰æ¼æ´ï¼Œå¯¼è‡´ç»“è®ºé”™è¯¯ã€‚"
            "\n\nè¾“å‡ºæ ¼å¼ï¼š"
            "\nè¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š"
            "\nçŠ¶æ€ï¼š[æ— å¹»è§‰/äº‹å®å†²çª/æ— ä¸­ç”Ÿæœ‰/æŒ‡ä»¤è¯¯è§£/é€»è¾‘é”™è¯¯]"
            "\nè¯¦ç»†è¯´æ˜ï¼š[å…·ä½“çš„é—®é¢˜æè¿°]"
        )
        
        user_prompt = (
            f"ç­”æ¡ˆ: {answer}\n"
            f"å¼•æ–‡: {combined_citations}\n"
            "è¯·æŒ‰ç…§è¦æ±‚æ ¼å¼ç»™å‡ºåˆ†æç»“æœã€‚"
        )
        
        return f"{system_prompt}\n\n{user_prompt}"

    def _parse_analysis_result(self, response_text: str, entry_id: int) -> tuple[str, str]:
        """
        è§£æAIåˆ†æç»“æœï¼Œæå–çŠ¶æ€å’Œè¯¦ç»†è¯´æ˜ï¼ˆå‚è€ƒåŸç‰ˆ analyze_entry.pyï¼‰
        
        Args:
            response_text: AIè¿”å›çš„åˆ†ææ–‡æœ¬
            entry_id: æ•°æ®ID
            
        Returns:
            (status, detail): çŠ¶æ€å’Œè¯¦ç»†è¯´æ˜çš„å…ƒç»„
        """
        # æ¸…ç†å“åº”æ–‡æœ¬
        response_text = response_text.strip()
        
        # å°è¯•ä»å“åº”ä¸­æå–çŠ¶æ€å’Œè¯¦ç»†è¯´æ˜
        if "æ— å¹»è§‰" in response_text:
            status = "æ— å¹»è§‰"
            # æå–è¯¦ç»†è¯´æ˜
            if "ï¼š" in response_text:
                detail = response_text.split("ï¼š", 1)[1].strip()
            else:
                detail = "ç­”æ¡ˆä¸å¼•æ–‡å†…å®¹ä¸€è‡´"
        elif "äº‹å®å†²çª" in response_text:
            status = "äº‹å®å†²çª"
            detail = self._extract_detail(response_text, "äº‹å®å†²çª")
        elif "æ— ä¸­ç”Ÿæœ‰" in response_text:
            status = "æ— ä¸­ç”Ÿæœ‰"
            detail = self._extract_detail(response_text, "æ— ä¸­ç”Ÿæœ‰")
        elif "æŒ‡ä»¤è¯¯è§£" in response_text:
            status = "æŒ‡ä»¤è¯¯è§£"
            detail = self._extract_detail(response_text, "æŒ‡ä»¤è¯¯è§£")
        elif "é€»è¾‘é”™è¯¯" in response_text:
            status = "é€»è¾‘é”™è¯¯"
            detail = self._extract_detail(response_text, "é€»è¾‘é”™è¯¯")
        else:
            # å…œåº•å¤„ç†ï¼šæ ¹æ®æ˜¯å¦åŒ…å«"æœ‰å¹»è§‰é—®é¢˜"æ¥åˆ¤æ–­
            if "æœ‰å¹»è§‰é—®é¢˜" in response_text:
                # é»˜è®¤ä¸º"æ— ä¸­ç”Ÿæœ‰"ç±»å‹
                status = "æ— ä¸­ç”Ÿæœ‰"
                detail = self._extract_detail(response_text, "æœ‰å¹»è§‰é—®é¢˜")
            else:
                status = "æ— å¹»è§‰"
                detail = "ç­”æ¡ˆä¸å¼•æ–‡å†…å®¹ä¸€è‡´"
        
        return status, detail

    def _extract_detail(self, response_text: str, keyword: str) -> str:
        """æå–è¯¦ç»†è¯´æ˜ï¼ˆå‚è€ƒåŸç‰ˆ analyze_entry.pyï¼‰"""
        try:
            if keyword in response_text:
                # æ‰¾åˆ°å…³é”®è¯åçš„å†…å®¹
                start_idx = response_text.find(keyword) + len(keyword)
                if "ï¼š" in response_text[start_idx:]:
                    detail = response_text[start_idx:].split("ï¼š", 1)[1].strip()
                else:
                    detail = response_text[start_idx:].strip()
                
                # æ¸…ç†å¤šä½™çš„æ ‡ç‚¹ç¬¦å·
                if detail.startswith("ï¼Œ"):
                    detail = detail[1:]
                if detail.startswith("ï¼š"):
                    detail = detail[1:]
                
                return detail if detail else f"æ£€æµ‹åˆ°{keyword}é—®é¢˜"
            else:
                return f"æ£€æµ‹åˆ°{keyword}é—®é¢˜"
        except:
            return f"æ£€æµ‹åˆ°{keyword}é—®é¢˜"

    async def analyze_single_item(self, session: aiohttp.ClientSession, item: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ•°æ®é¡¹"""
        try:
            # æå–æ•°æ® - é€‚é…ç°æœ‰é¡¹ç›®çš„åˆ—åæ ¼å¼
            question = ""
            answer = ""
            
            # ä½¿ç”¨ä¸ç°æœ‰citation_analyzerç›¸åŒçš„æ•°æ®æå–é€»è¾‘
            # å‚è€ƒ citation_analyzer_sliced.py:1022-1023
            
            # æå–é—®é¢˜å’Œç­”æ¡ˆ
            question = str(item.get('æ¨¡å‹prompt', '')) if pd.notna(item.get('æ¨¡å‹prompt', '')) else ""
            answer = str(item.get('ç­”æ¡ˆ', '')) if pd.notna(item.get('ç­”æ¡ˆ', '')) else ""
            
            # æå–å¼•æ–‡ - æŒ‰åŸç‰ˆé€»è¾‘åˆå¹¶æ‰€æœ‰å¼•æ–‡åˆ—ï¼ˆå‚è€ƒ preprocess_csv_tool.py:75-82ï¼‰
            citations_list = []
            for key, value in item.items():
                if pd.notna(value) and key.startswith('å¼•æ–‡') and str(value).strip():
                    citations_list.append(str(value).strip())
            
            # åˆå¹¶å¼•æ–‡ï¼Œç”¨ || åˆ†éš”ï¼ˆä¸åŸç‰ˆä¿æŒä¸€è‡´ï¼‰
            combined_citations = " || ".join(citations_list) if citations_list else ""
            
            # è°ƒè¯•ä¿¡æ¯ï¼šæŸ¥çœ‹æ•°æ®æå–æƒ…å†µ
            rank = item.get('rank', 0)
            print(f"ğŸ” ç¬¬{rank}æ¡æ•°æ®æå–ç»“æœ:")
            print(f"  é—®é¢˜é•¿åº¦: {len(question)}")
            print(f"  ç­”æ¡ˆé•¿åº¦: {len(answer)}")
            print(f"  å¼•æ–‡æ®µæ•°: {len(citations_list)}")
            print(f"  åˆå¹¶å¼•æ–‡é•¿åº¦: {len(combined_citations)}")
            if not answer:
                print(f"  âŒ ç­”æ¡ˆä¸ºç©º")
            if not combined_citations:
                citation_cols = [k for k in item.keys() if k.startswith('å¼•æ–‡')]
                print(f"  âŒ æ— å¼•æ–‡æ•°æ®ï¼Œå‘ç°å¼•æ–‡åˆ—: {len(citation_cols)}ä¸ª")
            
            # æŒ‰åŸç‰ˆé€»è¾‘ï¼šç­”æ¡ˆå’Œå¼•æ–‡éƒ½å¿…é¡»éç©ºï¼ˆå‚è€ƒ preprocess_csv_tool.py:85ï¼‰
            if not answer.strip() or not combined_citations.strip():
                return {
                    'rank': item.get('rank', 0),
                    'api_success': False,
                    'error': 'ç¼ºå°‘å¿…è¦çš„ç­”æ¡ˆæˆ–å¼•æ–‡æ•°æ®',
                    'hallucination_category': 'unknown',
                    'has_hallucination': None
                }
            
            # åˆ›å»ºæç¤ºè¯ï¼ˆä½¿ç”¨åˆå¹¶åçš„å¼•æ–‡ï¼‰
            prompt = self.create_hallucination_prompt(question, answer, combined_citations)
            
            # è°ƒç”¨API
            result = await self.api_client.call_async(session, prompt, temperature=0.1, max_tokens=4000)
            
            if result['success']:
                try:
                    # ä½¿ç”¨åŸç‰ˆçš„æ–‡æœ¬è§£ææ–¹å¼ï¼Œä¸æ˜¯JSONè§£æ
                    content = result['content'].strip()
                    
                    # è§£æå“åº”ï¼Œæå–çŠ¶æ€å’Œè¯¦ç»†è¯´æ˜ï¼ˆå‚è€ƒ analyze_entry.pyï¼‰
                    status, detail = self._parse_analysis_result(content, item.get('rank', 0))
                    
                    return {
                        'rank': item.get('rank', 0),
                        'api_success': True,
                        'id': item.get('rank', 0),
                        'status': status,
                        'detail': detail
                    }
                    
                except Exception as e:
                    return {
                        'rank': item.get('rank', 0),
                        'api_success': False,
                        'error': f'å“åº”è§£æå¤±è´¥: {e}',
                        'raw_response': content[:500] if 'content' in locals() else '',
                        'status': 'unknown',
                        'detail': f'è§£æå¼‚å¸¸: {str(e)}'
                    }
            else:
                return {
                    'rank': item.get('rank', 0),
                    'api_success': False,
                    'error': result['error'],
                    'hallucination_category': 'unknown',
                    'has_hallucination': None
                }
                
        except Exception as e:
            return {
                'rank': item.get('rank', 0),
                'api_success': False,
                'error': f'åˆ†æå¼‚å¸¸: {str(e)}',
                'hallucination_category': 'unknown',
                'has_hallucination': None
            }

    async def batch_analyze_excel(self, file_content: bytes, num_samples: int = None, 
                                 specific_rank: int = None, start_from: int = None) -> List[Dict[str, Any]]:
        """æ‰¹é‡åˆ†æExcelæ–‡ä»¶ä¸­çš„å¹»è§‰é—®é¢˜"""
        try:
            # è¯»å–Excelæ–‡ä»¶
            import io
            df = pd.read_excel(io.BytesIO(file_content))
            print(f"ğŸ“Š Excelæ–‡ä»¶è¯»å–æˆåŠŸï¼Œå…±{len(df)}è¡Œæ•°æ®")
            print(f"ğŸ“‹ æ£€æµ‹åˆ°çš„åˆ—å: {df.columns.tolist()}")
            
            # æ‰“å°å‰å‡ è¡Œæ•°æ®ç”¨äºè°ƒè¯•
            if len(df) > 0:
                print(f"ğŸ“ ç¬¬ä¸€è¡Œæ•°æ®ç¤ºä¾‹: {dict(df.iloc[0])}")
                print(f"ğŸ“ éç©ºåˆ—ç»Ÿè®¡: {df.count().to_dict()}")
            
            # æ ¹æ®å‚æ•°ç­›é€‰æ•°æ®
            if specific_rank:
                if specific_rank <= len(df):
                    df = df.iloc[[specific_rank - 1]]
                    print(f"ğŸ” åˆ†æç¬¬{specific_rank}æ¡æ•°æ®")
                else:
                    raise ValueError(f"æŒ‡å®šçš„rank {specific_rank} è¶…å‡ºæ•°æ®èŒƒå›´ï¼ˆå…±{len(df)}è¡Œï¼‰")
            elif start_from:
                start_idx = start_from - 1
                if num_samples:
                    end_idx = start_idx + num_samples
                    df = df.iloc[start_idx:end_idx]
                    print(f"ğŸ” åˆ†æä»ç¬¬{start_from}æ¡å¼€å§‹çš„{num_samples}æ¡æ•°æ®")
                else:
                    df = df.iloc[start_idx:]
                    print(f"ğŸ” åˆ†æä»ç¬¬{start_from}æ¡åˆ°ç»“å°¾çš„æ•°æ®")
            elif num_samples:
                df = df.head(num_samples)
                print(f"ğŸ” åˆ†æå‰{num_samples}æ¡æ•°æ®")
            else:
                print(f"ğŸ” åˆ†ææ‰€æœ‰{len(df)}æ¡æ•°æ®")
            
            # ä¸ºæ¯è¡Œæ•°æ®æ·»åŠ rankä¿¡æ¯
            data_items = []
            for idx, row in df.iterrows():
                item = row.to_dict()
                item['rank'] = idx + 1  # rankä»1å¼€å§‹
                data_items.append(item)
            
            # å¼‚æ­¥å¹¶å‘åˆ†æ
            semaphore = asyncio.Semaphore(self.concurrent_limit)
            
            async def analyze_with_semaphore(item):
                async with semaphore:
                    async with aiohttp.ClientSession() as session:
                        return await self.analyze_single_item(session, item)
            
            print(f"ğŸ”„ å¼€å§‹å¹¶å‘å¹»è§‰æ£€æµ‹åˆ†æï¼Œå¹¶å‘é™åˆ¶: {self.concurrent_limit}")
            tasks = [analyze_with_semaphore(item) for item in data_items]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # å¤„ç†å¼‚å¸¸ç»“æœ
            final_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    final_results.append({
                        'rank': data_items[i].get('rank', i + 1),
                        'api_success': False,
                        'error': f'åˆ†æå¼‚å¸¸: {str(result)}',
                        'hallucination_category': 'unknown',
                        'has_hallucination': None
                    })
                else:
                    final_results.append(result)
            
            print(f"âœ… å¹»è§‰æ£€æµ‹åˆ†æå®Œæˆï¼Œå…±{len(final_results)}æ¡ç»“æœ")
            return final_results
            
        except Exception as e:
            print(f"âŒ æ‰¹é‡åˆ†æå¤±è´¥: {str(e)}")
            raise

    def save_results(self, results: List[Dict[str, Any]], output_path: str):
        """ä¿å­˜åˆ†æç»“æœ"""
        try:
            import os
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… å¹»è§‰æ£€æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {str(e)}")

    def generate_summary(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆåˆ†ææ‘˜è¦ï¼Œä½¿ç”¨åŸç‰ˆçš„statuså­—æ®µ"""
        total_count = len(results)
        success_count = sum(1 for r in results if r.get('api_success', False))
        failed_count = total_count - success_count
        
        # ç»Ÿè®¡å¹»è§‰ç±»åˆ«ï¼ˆä½¿ç”¨statuså­—æ®µï¼‰
        category_stats = {}
        no_hallucination_count = 0
        
        for result in results:
            if result.get('api_success'):
                status = result.get('status', 'unknown')
                category_stats[status] = category_stats.get(status, 0) + 1
                
                if status == 'æ— å¹»è§‰':
                    no_hallucination_count += 1
        
        hallucination_count = success_count - no_hallucination_count
        
        return {
            'total_count': total_count,
            'success_count': success_count,
            'failed_count': failed_count,
            'hallucination_count': hallucination_count,
            'no_hallucination_count': no_hallucination_count,
            'hallucination_rate': hallucination_count / success_count if success_count > 0 else 0,
            'category_distribution': category_stats
        }