from fastapi import FastAPI, Request, UploadFile, File
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import os
import asyncio

# Import the logic classes
from .logic.citation_analyzer_fulltext import Method1BailianAnalyzer as CitationAnalyzer
from .logic.citation_analyzer_sliced import ConsistencyEvaluator as SlicedAnalyzer

app = FastAPI()

# --- App Setup ---
# Mount the static directory to serve frontend files
static_dir = os.path.join(os.path.dirname(__file__), "..", "static")
app.mount("/static", StaticFiles(directory=static_dir), name="static")

# Initialize analyzers
# It's better to initialize them once when the app starts
print("ğŸš€ æ­£åœ¨åˆå§‹åŒ–åˆ†æå™¨...")
try:
    citation_analyzer = CitationAnalyzer()
    print("âœ… Fulltextåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    print(f"âŒ Fulltextåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
    import traceback
    traceback.print_exc()

try:
    sliced_analyzer = SlicedAnalyzer()
    print("âœ… Slicedåˆ†æå™¨åˆå§‹åŒ–æˆåŠŸ")
except Exception as e:
    print(f"âŒ Slicedåˆ†æå™¨åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
    import traceback
    traceback.print_exc()

print("ğŸ¯ åˆ†æå™¨åˆå§‹åŒ–å®Œæˆ")

# --- Pydantic Models ---
class AnalysisRequest(BaseModel):
    text: str
    question: str = "" # Optional question context
    # A dictionary where key is citation number (int) and value is the text (str)
    citations: dict[int, str] = {}

# --- API Endpoints ---
@app.get("/", response_class=HTMLResponse)
async def read_root():
    """Serve the main index.html file."""
    index_path = os.path.join(static_dir, "index.html")
    with open(index_path, "r", encoding="utf-8") as f:
        return HTMLResponse(content=f.read())

@app.post("/api/analyze")
async def analyze_text(request: AnalysisRequest):
    """
    This endpoint receives text and performs multiple analyses.
    It runs citation and consistency analysis concurrently.
    """
    # Create concurrent tasks for each analysis
    citation_task = citation_analyzer.analyze(
        question=request.question,
        answer=request.text,
        citations_dict=request.citations
    )
    
    consistency_task = asyncio.to_thread(sliced_analyzer.evaluate, request.text)

    # Run tasks concurrently and wait for results
    citation_results, consistency_results = await asyncio.gather(
        citation_task,
        consistency_task
    )

    # Combine results into a single response
    return JSONResponse(content={
        "original_text_preview": request.text[:200] + "...",
        "citation_analysis": citation_results,
        "consistency_evaluation": consistency_results,
        # Placeholder for hallucination detection
        "hallucination_detection": "Not implemented yet."
    })

@app.post("/api/analyze-xlsx")
async def analyze_xlsx_file(
    request: Request,
    file: UploadFile = File(...)
):
    """
    ä¸Šä¼ xlsxæ–‡ä»¶è¿›è¡Œæ‰¹é‡å¼•æ–‡åˆ†æ
    """
    print("ğŸš¨ğŸš¨ğŸš¨ APIå‡½æ•° analyze_xlsx_file è¢«è°ƒç”¨äº†ï¼ğŸš¨ğŸš¨ğŸš¨")
    
    # è·å–APIé…ç½®å’Œåˆ†æç±»å‹
    api_key = request.headers.get('X-API-Key')
    api_provider = request.headers.get('X-API-Provider', 'alibaba')
    api_model = request.headers.get('X-API-Model', '')
    api_base_url = request.headers.get('X-API-Base-URL', '')
    analysis_type = request.headers.get('X-Analysis-Type', 'fulltext')
    
    print(f"ğŸ”‘ APIé…ç½®: å¯†é’¥={'å·²è®¾ç½®' if api_key else 'æœªè®¾ç½®'}, æä¾›å•†={api_provider}, æ¨¡å‹={api_model or 'é»˜è®¤'}, åˆ†æç±»å‹={analysis_type}")
    
    if not api_key:
        return JSONResponse(
            status_code=400,
            content={"error": "ç¼ºå°‘API Key"}
        )
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    if not file.filename.endswith('.xlsx'):
        return JSONResponse(
            status_code=400,
            content={"error": "åªæ”¯æŒxlsxæ ¼å¼æ–‡ä»¶"}
        )
    
    try:
        print("ğŸŒŸ APIç«¯ç‚¹è¢«è°ƒç”¨")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        file_content = await file.read()
        print(f"ğŸ“ æ–‡ä»¶è¯»å–å®Œæˆï¼Œå¤§å°ï¼š{len(file_content)} bytes")
        
        # è·å–åˆ†æé€‰é¡¹
        analysis_mode = request.headers.get('X-Analysis-Mode', 'all')  # all, head, specific, range
        num_samples = request.headers.get('X-Num-Samples')
        specific_rank = request.headers.get('X-Specific-Rank')  
        start_from = request.headers.get('X-Start-From')
        
        print(f"âš™ï¸ åˆ†æé€‰é¡¹ï¼štype={analysis_type}, mode={analysis_mode}, samples={num_samples}, rank={specific_rank}, start={start_from}")
        
        # è½¬æ¢æ•°å€¼å‚æ•°
        try:
            if num_samples:
                num_samples = int(num_samples)
            if specific_rank:
                specific_rank = int(specific_rank)
            if start_from:
                start_from = int(start_from)
        except ValueError:
            print("âŒ å‚æ•°æ ¼å¼é”™è¯¯")
            return JSONResponse(
                status_code=400,
                content={"error": "åˆ†æå‚æ•°æ ¼å¼é”™è¯¯"}
            )
        
        # æ ¹æ®åˆ†æç±»å‹é€‰æ‹©åˆ†æå™¨
        if analysis_type == 'sliced':
            # ä½¿ç”¨slicedç‰ˆæœ¬åˆ†æå™¨ï¼Œæ”¯æŒå¤šAPIæä¾›å•†
            analyzer = SlicedAnalyzer(
                api_key=api_key,
                provider=api_provider,
                base_url=api_base_url if api_base_url else None,
                model=api_model if api_model else None
            )
            results = await analyzer.analyze_xlsx_file(
                file_content=file_content,
                filename=file.filename
            )
        else:
            # ä½¿ç”¨fulltextç‰ˆæœ¬åˆ†æå™¨ï¼ˆé»˜è®¤ï¼‰
            citation_analyzer.api_key = api_key  # è®¾ç½®API Key
            
            # ç›´æ¥è°ƒç”¨è„šæœ¬æ–¹æ³•å¹¶ä»ä¿å­˜çš„æ–‡ä»¶è¯»å–ç»“æœ
            import tempfile
            import json
            import uuid
            
            # åˆ›å»ºä¸´æ—¶Excelæ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as temp_excel:
                temp_excel.write(file_content)
                temp_excel_path = temp_excel.name
            
            # åˆ›å»ºä¸´æ—¶JSONè¾“å‡ºæ–‡ä»¶
            import tempfile
            temp_json_fd, temp_json_path = tempfile.mkstemp(suffix='.json', prefix='web_analysis_')
            os.close(temp_json_fd)  # å…³é—­æ–‡ä»¶æè¿°ç¬¦ï¼Œè®©è„šæœ¬å¯ä»¥å†™å…¥æ–‡ä»¶
            
            try:
                print("ğŸ”„ å¼€å§‹è°ƒç”¨fulltextåˆ†æè„šæœ¬...")
                
                # æ ¹æ®åˆ†ææ¨¡å¼è°ƒç”¨è„šæœ¬æ–¹æ³•
                if analysis_mode == 'head' and num_samples:
                    print(f"ğŸ“Š åˆ†ææ¨¡å¼ï¼šå‰{num_samples}æ¡")
                    script_results = await citation_analyzer.batch_analyze_concurrent(
                        temp_excel_path, num_samples=num_samples
                    )
                elif analysis_mode == 'specific' and specific_rank:
                    print(f"ğŸ“Š åˆ†ææ¨¡å¼ï¼šç¬¬{specific_rank}æ¡")
                    script_results = await citation_analyzer.batch_analyze_concurrent(
                        temp_excel_path, specific_rank=specific_rank
                    )
                elif analysis_mode == 'range' and start_from:
                    print(f"ğŸ“Š åˆ†ææ¨¡å¼ï¼šä»ç¬¬{start_from}æ¡å¼€å§‹ï¼Œ{num_samples or 'å…¨éƒ¨'}æ¡")
                    script_results = await citation_analyzer.batch_analyze_concurrent(
                        temp_excel_path, start_from=start_from, num_samples=num_samples
                    )
                else:
                    # é»˜è®¤åˆ†ææ‰€æœ‰æ•°æ®
                    print("ğŸ“Š åˆ†ææ¨¡å¼ï¼šå…¨éƒ¨æ•°æ®")
                    script_results = await citation_analyzer.batch_analyze_concurrent(temp_excel_path)
                
                print(f"âœ… è„šæœ¬åˆ†æå®Œæˆï¼Œè·å¾—{len(script_results)}æ¡ç»“æœ")
                
                # ä½¿ç”¨è„šæœ¬çš„ä¿å­˜æ–¹æ³•ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                print("ğŸ’¾ ä¿å­˜ä¸´æ—¶ç»“æœæ–‡ä»¶...")
                citation_analyzer.save_results(script_results, temp_json_path)
                
                # åŒæ—¶ä¿å­˜åˆ°é¡¹ç›®çš„data/output/resultsç›®å½•
                import datetime
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                permanent_output_path = os.path.join(
                    os.path.dirname(__file__), "..", "data", "output", "results",
                    f"web_analysis_{analysis_type}_{analysis_mode}_{timestamp}.json"
                )
                
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                print(f"ğŸ“ å‡†å¤‡ä¿å­˜åˆ°ï¼š{permanent_output_path}")
                os.makedirs(os.path.dirname(permanent_output_path), exist_ok=True)
                
                # ä¿å­˜æ°¸ä¹…å‰¯æœ¬
                citation_analyzer.save_results(script_results, permanent_output_path)
                print(f"âœ… Webåˆ†æç»“æœå·²ä¿å­˜åˆ°ï¼š{permanent_output_path}")
                
                # ä»ä¿å­˜çš„æ–‡ä»¶è¯»å–å¹²å‡€çš„æ•°æ®
                print("ğŸ“– ä»æ–‡ä»¶è¯»å–ç»“æœ...")
                with open(temp_json_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                
                print(f"âœ… æˆåŠŸè¯»å–{len(results)}æ¡ç»“æœ")
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_json_path)
                    print("ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                except:
                    pass
                    
            except Exception as e:
                print(f"âŒ Fulltextåˆ†æè¿‡ç¨‹å‡ºé”™ï¼š{str(e)}")
                import traceback
                traceback.print_exc()
                raise ValueError(f"Fulltextåˆ†ææ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
            finally:
                # æ¸…ç†ä¸´æ—¶Excelæ–‡ä»¶
                try:
                    os.unlink(temp_excel_path)
                except:
                    pass
        
        # æ·»åŠ åˆ†æé€‰é¡¹ä¿¡æ¯åˆ°ç»“æœä¸­
        analysis_info = {
            'analysis_type': analysis_type,
            'analysis_mode': analysis_mode
        }
        if num_samples:
            analysis_info['num_samples'] = num_samples
        if specific_rank:
            analysis_info['specific_rank'] = specific_rank
        if start_from:
            analysis_info['start_from'] = start_from
        
        # ç»Ÿè®¡ç»“æœ
        total_count = len(results)
        success_count = sum(1 for r in results if r.get('api_success', False))
        failed_count = total_count - success_count
        
        # æ¸…ç†è¿”å›ç»“æœä¸­çš„æ— æ•ˆæµ®ç‚¹æ•°
        def clean_for_json(obj):
            """é€’å½’æ¸…ç†å¯¹è±¡ä¸­çš„æ— æ•ˆæµ®ç‚¹æ•°"""
            import math
            import pandas as pd
            import numpy as np
            
            if isinstance(obj, dict):
                return {key: clean_for_json(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [clean_for_json(item) for item in obj]
            elif isinstance(obj, (float, np.floating)):
                if math.isnan(obj) or math.isinf(obj):
                    return None
                return float(obj)
            elif isinstance(obj, (np.integer, np.int64)):
                return int(obj)
            elif hasattr(obj, 'item'):  # numpyæ ‡é‡
                return clean_for_json(obj.item())
            else:
                # ä½¿ç”¨pandasæ£€æŸ¥NaN/NA
                try:
                    if pd.isna(obj):
                        return None
                except (TypeError, ValueError):
                    pass
                return obj
        
        # æ¸…ç†å“åº”æ•°æ®
        response_data = {
            "filename": file.filename,
            "analysis_type": analysis_type,
            "analysis_mode": analysis_mode,
            "total_rows": total_count,
            "success_count": success_count,
            "failed_count": failed_count,
            "results": results[:10] if len(results) > 10 else results,
            "full_results_available": len(results) > 10,
            "output_file_saved": permanent_output_path if 'permanent_output_path' in locals() else None
        }
        
        cleaned_response = clean_for_json(response_data)
        return JSONResponse(content=cleaned_response)
        
    except ValueError as e:
        return JSONResponse(
            status_code=400,
            content={"error": str(e)}
        )
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"}
        )
