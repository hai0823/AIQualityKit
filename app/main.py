from fastapi import FastAPI, Request, UploadFile, File
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import os
import asyncio

# Import the logic classes
from .logic.citation_analyzer_fulltext import Method1BailianAnalyzer as CitationAnalyzer
from .logic.citation_analyzer_sync import ConsistencyEvaluator as SyncAnalyzer
from .logic.citation_analyzer_async import ConsistencyEvaluator as AsyncAnalyzer
from .logic.internal_consistency_detector import InternalConsistencyDetector

app = FastAPI()

# --- App Setup ---
# Mount the static directory to serve frontend files
static_dir = os.path.join(os.path.dirname(__file__), "..", "static")
app.mount("/static", StaticFiles(directory=static_dir), name="static")

# Analyzers will be initialized at runtime with API keys

# --- Pydantic Models ---
class AnalysisRequest(BaseModel):
    text: str
    question: str = "" # Optional question context
    # A dictionary where key is citation number (int) and value is the text (str)
    citations: dict[int, str] = {}

# --- API Endpoints ---
@app.get("/", response_class=HTMLResponse)
async def read_root():
    """Serve the main index.html file."""
    try:
        index_path = os.path.join(static_dir, "index.html")
        print(f"ğŸ” å°è¯•è¯»å–æ–‡ä»¶: {index_path}")
        
        if not os.path.exists(index_path):
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {index_path}")
            return HTMLResponse(content="<h1>é”™è¯¯ï¼šindex.htmlæ–‡ä»¶ä¸å­˜åœ¨</h1>", status_code=404)
        
        with open(index_path, "r", encoding="utf-8") as f:
            content = f.read()
            print(f"âœ… æˆåŠŸè¯»å–index.htmlï¼Œå¤§å°: {len(content)} å­—ç¬¦")
            return HTMLResponse(content=content)
            
    except Exception as e:
        print(f"âŒ è¯»å–index.htmlå‡ºé”™: {str(e)}")
        import traceback
        traceback.print_exc()
        return HTMLResponse(content=f"<h1>é”™è¯¯ï¼šæ— æ³•è¯»å–index.html</h1><p>{str(e)}</p>", status_code=500)

@app.post("/api/analyze")
async def analyze_text(request: AnalysisRequest, http_request: Request):
    """
    This endpoint receives text and performs multiple analyses.
    It runs citation and consistency analysis concurrently.
    """
    # è·å–APIé…ç½®
    api_key = http_request.headers.get('X-API-Key')
    api_provider = http_request.headers.get('X-API-Provider', 'alibaba')
    api_model = http_request.headers.get('X-API-Model', '')
    api_base_url = http_request.headers.get('X-API-Base-URL', '')
    
    if not api_key:
        return JSONResponse(
            status_code=400,
            content={"error": "ç¼ºå°‘API Keyï¼Œè¯·åœ¨è¯·æ±‚å¤´ä¸­æä¾›X-API-Key"}
        )
    
    try:
        # ä½¿ç”¨è¿è¡Œæ—¶API Keyåˆå§‹åŒ–å¼•æ–‡åˆ†æå™¨
        citation_analyzer = CitationAnalyzer(
            api_key=api_key,
            provider=api_provider,
            model=api_model if api_model else None,
            base_url=api_base_url if api_base_url else None
        )
        
        # è¿è¡Œå¼•æ–‡åˆ†æ
        citation_results = await citation_analyzer.analyze(
            question=request.question,
            answer=request.text,
            citations_dict=request.citations
        )

        # Combine results into a single response
        return JSONResponse(content={
            "original_text_preview": request.text[:200] + "...",
            "citation_analysis": citation_results,
            "consistency_evaluation": "å·²ç§»é™¤æ—§ç‰ˆslicedåˆ†æå™¨ï¼Œè¯·ä½¿ç”¨æ–°ç‰ˆevaluator",
            # Placeholder for hallucination detection
            "hallucination_detection": "Not implemented yet."
        })
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"åˆ†æå¤±è´¥ï¼š{str(e)}"}
        )

@app.post("/api/analyze-xlsx")
async def analyze_xlsx_file(
    request: Request,
    file: UploadFile = File(...)
):
    """
    ä¸Šä¼ xlsxæ–‡ä»¶è¿›è¡Œæ‰¹é‡å¼•æ–‡åˆ†æ
    """
    print("ğŸš¨ğŸš¨ğŸš¨ APIå‡½æ•° analyze_xlsx_file è¢«è°ƒç”¨äº†ï¼ğŸš¨ğŸš¨ğŸš¨")
    
    # è·å–APIé…ç½®å’Œåˆ†æç±»å‹
    api_key = request.headers.get('X-API-Key')
    api_provider = request.headers.get('X-API-Provider', 'alibaba')
    api_model = request.headers.get('X-API-Model', '')
    api_base_url = request.headers.get('X-API-Base-URL', '')
    analysis_type = request.headers.get('X-Analysis-Type', 'fulltext')
    
    print(f"ğŸ”‘ APIé…ç½®: å¯†é’¥={'å·²è®¾ç½®' if api_key else 'æœªè®¾ç½®'}, æä¾›å•†={api_provider}, æ¨¡å‹={api_model or 'é»˜è®¤'}, åˆ†æç±»å‹={analysis_type}")
    
    if not api_key:
        return JSONResponse(
            status_code=400,
            content={"error": "ç¼ºå°‘API Key"}
        )
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    if not file.filename.endswith('.xlsx'):
        return JSONResponse(
            status_code=400,
            content={"error": "åªæ”¯æŒxlsxæ ¼å¼æ–‡ä»¶"}
        )
    
    try:
        print("ğŸŒŸ APIç«¯ç‚¹è¢«è°ƒç”¨")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        file_content = await file.read()
        print(f"ğŸ“ æ–‡ä»¶è¯»å–å®Œæˆï¼Œå¤§å°ï¼š{len(file_content)} bytes")
        
        # è·å–åˆ†æé€‰é¡¹
        analysis_mode = request.headers.get('X-Analysis-Mode', 'all')  # all, head, specific, range
        num_samples = request.headers.get('X-Num-Samples')
        specific_rank = request.headers.get('X-Specific-Rank')  
        start_from = request.headers.get('X-Start-From')
        
        print(f"âš™ï¸ åˆ†æé€‰é¡¹ï¼štype={analysis_type}, mode={analysis_mode}, samples={num_samples}, rank={specific_rank}, start={start_from}")
        
        # è½¬æ¢æ•°å€¼å‚æ•°
        try:
            if num_samples:
                num_samples = int(num_samples)
            if specific_rank:
                specific_rank = int(specific_rank)
            if start_from:
                start_from = int(start_from)
        except ValueError:
            print("âŒ å‚æ•°æ ¼å¼é”™è¯¯")
            return JSONResponse(
                status_code=400,
                content={"error": "åˆ†æå‚æ•°æ ¼å¼é”™è¯¯"}
            )
        
        # æ ¹æ®åˆ†æç±»å‹é€‰æ‹©åˆ†æå™¨
        if analysis_type == 'sliced':
            # è·å–æ‰§è¡Œæ¨¡å¼ï¼ˆåŒæ­¥æˆ–å¼‚æ­¥ï¼‰
            execution_mode = request.headers.get('X-Execution-Mode', 'sync')
            print(f"ğŸ”„ ä½¿ç”¨Slicedåˆ†æï¼Œæ‰§è¡Œæ¨¡å¼: {execution_mode}")
            
            if execution_mode == 'sync':
                # ä½¿ç”¨åŒæ­¥ç‰ˆanalyzer
                print("ğŸ”„ ä½¿ç”¨åŒæ­¥ç‰ˆSliced Analyzerè¿›è¡Œåˆ†æ...")
                analyzer = SyncAnalyzer(
                    provider=api_provider,
                    model=api_model if api_model else None,
                    api_key=api_key,
                    base_url=api_base_url if api_base_url else None
                )
            else:
                # ä½¿ç”¨å¼‚æ­¥ç‰ˆanalyzer
                print("ğŸ”„ ä½¿ç”¨å¼‚æ­¥ç‰ˆSliced Analyzerè¿›è¡Œåˆ†æ...")
                analyzer = AsyncAnalyzer(
                    provider=api_provider,
                    model=api_model if api_model else None,
                    api_key=api_key,
                    base_url=api_base_url if api_base_url else None
                )
            
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è¿›è¡Œåˆ†æ
            import tempfile
            import json
            
            # åˆ›å»ºä¸´æ—¶Excelæ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as temp_excel:
                temp_excel.write(file_content)
                temp_excel_path = temp_excel.name
            
            try:
                print(f"ğŸ”„ å¼€å§‹ä½¿ç”¨{execution_mode}æ¨¡å¼Sliced Analyzerè¿›è¡ŒçœŸå®åˆ†æ...")
                
                # è¯»å–Excelæ–‡ä»¶æ•°æ®
                import pandas as pd
                import io
                
                try:
                    # å°†æ–‡ä»¶å†…å®¹è½¬æ¢ä¸ºDataFrame
                    df = pd.read_excel(io.BytesIO(file_content))
                    print(f"ğŸ“Š Excelæ–‡ä»¶è¯»å–æˆåŠŸï¼Œå…±{len(df)}è¡Œæ•°æ®")
                    
                    # æ ¹æ®åˆ†ææ¨¡å¼ç­›é€‰æ•°æ®
                    if analysis_mode == 'head' and num_samples:
                        df_to_analyze = df.head(num_samples)
                        print(f"ğŸ” åˆ†æå‰{num_samples}æ¡æ•°æ®")
                    elif analysis_mode == 'specific' and specific_rank:
                        if specific_rank <= len(df):
                            df_to_analyze = df.iloc[[specific_rank - 1]]  # -1 å› ä¸ºç´¢å¼•ä»0å¼€å§‹
                            print(f"ğŸ” åˆ†æç¬¬{specific_rank}æ¡æ•°æ®")
                        else:
                            raise ValueError(f"æŒ‡å®šçš„rank {specific_rank} è¶…å‡ºæ•°æ®èŒƒå›´ï¼ˆå…±{len(df)}è¡Œï¼‰")
                    elif analysis_mode == 'range' and start_from:
                        start_idx = start_from - 1  # -1 å› ä¸ºç´¢å¼•ä»0å¼€å§‹
                        if num_samples:
                            end_idx = start_idx + num_samples
                            df_to_analyze = df.iloc[start_idx:end_idx]
                            print(f"ğŸ” åˆ†æä»ç¬¬{start_from}æ¡å¼€å§‹çš„{num_samples}æ¡æ•°æ®")
                        else:
                            df_to_analyze = df.iloc[start_idx:]
                            print(f"ğŸ” åˆ†æä»ç¬¬{start_from}æ¡åˆ°ç»“å°¾çš„æ•°æ®")
                    else:
                        df_to_analyze = df
                        print(f"ğŸ” åˆ†ææ‰€æœ‰{len(df)}æ¡æ•°æ®")
                    
                    print(f"ğŸ“ˆ å®é™…åˆ†ææ•°æ®è¡Œæ•°ï¼š{len(df_to_analyze)}")
                    
                    # çœŸæ­£è°ƒç”¨analyzerè¿›è¡Œåˆ†æ
                    import datetime
                    analysis_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    
                    # ä½¿ç”¨citation_processorå¤„ç†Excelæ–‡ä»¶ï¼Œæå–å¼•æ–‡æ ‡æ³¨
                    from .logic.citation_processor import process_excel_file
                    
                    # ä¿å­˜ä¸´æ—¶Excelæ–‡ä»¶å†…å®¹åˆ°ç£ç›˜ä¾›citation_processorä½¿ç”¨
                    temp_excel_for_processing_fd, temp_excel_for_processing = tempfile.mkstemp(suffix='.xlsx', prefix='excel_for_processing_')
                    with os.fdopen(temp_excel_for_processing_fd, 'wb') as f:
                        f.write(file_content)
                    
                    print("ğŸ”„ ä½¿ç”¨citation_processoræå–å¼•æ–‡æ ‡æ³¨...")
                    try:
                        # ä½¿ç”¨citation_processoræå–å¼•æ–‡æ ‡æ³¨çš„å¥å­
                        citation_results = process_excel_file(temp_excel_for_processing)
                        print(f"ğŸ“Š æå–åˆ°{len(citation_results)}ä¸ªåŒ…å«å¼•æ–‡æ ‡æ³¨çš„å¥å­")
                        
                        # æ ¹æ®åˆ†ææ¨¡å¼ç­›é€‰citation_results
                        if analysis_mode == 'head' and num_samples:
                            # è·å–å‰Nä¸ªrankçš„æ•°æ®
                            citation_results = [r for r in citation_results if r['rank'] <= num_samples]
                        elif analysis_mode == 'specific' and specific_rank:
                            # è·å–ç‰¹å®šrankçš„æ•°æ®
                            citation_results = [r for r in citation_results if r['rank'] == specific_rank]
                        elif analysis_mode == 'range' and start_from:
                            # è·å–èŒƒå›´å†…çš„æ•°æ®
                            if num_samples:
                                end_rank = start_from + num_samples - 1
                                citation_results = [r for r in citation_results if start_from <= r['rank'] <= end_rank]
                            else:
                                citation_results = [r for r in citation_results if r['rank'] >= start_from]
                        
                        print(f"ğŸ“ˆ ç­›é€‰åå¾…åˆ†ææ•°æ®ï¼š{len(citation_results)}æ¡")
                        
                    except Exception as e:
                        print(f"âŒ citation_processorå¤„ç†å¤±è´¥ï¼š{e}")
                        citation_results = []
                    finally:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        try:
                            os.unlink(temp_excel_for_processing)
                        except:
                            pass
                    
                    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
                    import tempfile
                    import json
                    
                    # åˆ›å»ºä¸´æ—¶citation_results.jsonæ–‡ä»¶
                    temp_citation_fd, temp_citation_path = tempfile.mkstemp(suffix='.json', prefix='citation_results_')
                    with os.fdopen(temp_citation_fd, 'w', encoding='utf-8') as f:
                        json.dump(citation_results, f, ensure_ascii=False, indent=2)
                    
                    try:
                        # ç¡®å®šåˆ†æèŒƒå›´
                        if analysis_mode == 'head' and num_samples:
                            rank_start, rank_end = 1, num_samples
                        elif analysis_mode == 'specific' and specific_rank:
                            rank_start, rank_end = specific_rank, specific_rank
                        elif analysis_mode == 'range' and start_from:
                            if num_samples:
                                rank_start, rank_end = start_from, start_from + num_samples - 1
                            else:
                                rank_start, rank_end = start_from, len(df_to_analyze)
                        else:
                            rank_start, rank_end = 1, len(df_to_analyze)
                        
                        print(f"ğŸ” è°ƒç”¨analyzeråˆ†æ rank {rank_start} åˆ° {rank_end}")
                        
                        # è°ƒç”¨analyzer
                        if execution_mode == 'sync':
                            # åŒæ­¥æ¨¡å¼
                            print("ğŸ”„ å¼€å§‹åŒæ­¥åˆ†æ...")
                            analyzer_results = analyzer.evaluate_consistency(
                                citation_file=temp_citation_path,
                                excel_file=temp_excel_path,
                                rank_start=rank_start,
                                rank_end=rank_end
                            )
                        else:
                            # å¼‚æ­¥æ¨¡å¼
                            print("ğŸ”„ å¼€å§‹å¼‚æ­¥åˆ†æ...")
                            analyzer_results = await analyzer.evaluate_consistency_async(
                                citation_file=temp_citation_path,
                                excel_file=temp_excel_path,
                                rank_start=rank_start,
                                rank_end=rank_end
                            )
                        
                        # å¤„ç†analyzerç»“æœ
                        results = []
                        if analyzer_results and isinstance(analyzer_results, list):
                            for result in analyzer_results:
                                # æ„å»ºåˆ†æç»“æœæ–‡æœ¬
                                analysis_text = f"ä¸€è‡´æ€§è¯„ä¼°ï¼š{result.get('consistency', 'æœªçŸ¥')}\n"
                                analysis_text += f"å¼•ç”¨å†…å®¹ï¼š{result.get('citation_topic', 'æ— ')}\n"
                                analysis_text += f"åˆ†æåŸå› ï¼š{result.get('reason', 'æ— ')}"
                                
                                # è®¡ç®—ä¸€è‡´æ€§åˆ†æ•°ï¼ˆä¸€è‡´=1.0ï¼Œä¸ä¸€è‡´=0.0ï¼‰
                                consistency = result.get('consistency', '')
                                consistency_score = 1.0 if consistency == 'ä¸€è‡´' else 0.0
                                
                                processed_result = {
                                    "rank": result.get("rank", 0),
                                    "api_success": True,  # å¦‚æœè¿”å›äº†ç»“æœè¯´æ˜APIè°ƒç”¨æˆåŠŸ
                                    "analysis_type": "sliced",
                                    "execution_mode": execution_mode,
                                    "analysis_mode": analysis_mode,
                                    "provider": api_provider,
                                    "model": api_model or "default",
                                    "analysis_time": analysis_time,
                                    "message": f"Slicedåˆ†æå®Œæˆ - {execution_mode}æ¨¡å¼ï¼Œç¬¬{result.get('rank', 0)}è¡Œ",
                                    "status": "success",
                                    "question": result.get("topic", "")[:200],
                                    "analysis": analysis_text,
                                    "citations_found": result.get("citation_numbers", []),
                                    "consistency_score": consistency_score,
                                    "processing_time": "1.0s"  # ç®€åŒ–å¤„ç†æ—¶é—´æ˜¾ç¤º
                                }
                                results.append(processed_result)
                        else:
                            # å¦‚æœanalyzeræ²¡æœ‰è¿”å›ç»“æœï¼Œåˆ›å»ºé”™è¯¯ä¿¡æ¯
                            results = [{
                                "rank": 1,
                                "api_success": False,
                                "analysis_type": "sliced",
                                "execution_mode": execution_mode,
                                "status": "failed",
                                "message": "Analyzerè°ƒç”¨å¤±è´¥",
                                "error": "Analyzeræœªè¿”å›æœ‰æ•ˆç»“æœ",
                                "analysis": f"Sliced analyzerè°ƒç”¨å¤±è´¥ï¼Œå¯èƒ½çš„åŸå› ï¼š\n1. æ•°æ®æ ¼å¼ä¸åŒ¹é…\n2. APIè°ƒç”¨å¤±è´¥\n3. æ–‡ä»¶å¤„ç†é”™è¯¯"
                            }]
                        
                        print(f"âœ… Sliced analyzeråˆ†æå®Œæˆï¼Œè·å¾—{len(results)}æ¡ç»“æœ")
                        
                    finally:
                        # æ¸…ç†ä¸´æ—¶citationæ–‡ä»¶
                        try:
                            os.unlink(temp_citation_path)
                        except:
                            pass
                    
                except Exception as e:
                    print(f"âŒ Excelæ–‡ä»¶å¤„ç†å¤±è´¥ï¼š{str(e)}")
                    # å¦‚æœExcelå¤„ç†å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯
                    results = [{
                        "rank": 1,
                        "api_success": False,
                        "analysis_type": "sliced",
                        "execution_mode": execution_mode,
                        "status": "failed",
                        "message": "Excelæ–‡ä»¶å¤„ç†å¤±è´¥",
                        "error": str(e),
                        "analysis": f"æ— æ³•å¤„ç†Excelæ–‡ä»¶ï¼š{str(e)}\n\nè¯·ç¡®ä¿æ–‡ä»¶æ ¼å¼æ­£ç¡®ï¼ŒåŒ…å«å¿…è¦çš„åˆ—ï¼šæ¨¡å‹promptã€ç­”æ¡ˆã€å¼•æ–‡1-å¼•æ–‡20"
                    }]
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_excel_path)
                except:
                    pass
        else:
            # ä½¿ç”¨fulltextç‰ˆæœ¬åˆ†æå™¨ï¼ˆé»˜è®¤ï¼‰
            citation_analyzer = CitationAnalyzer(
                api_key=api_key,
                provider=api_provider,
                model=api_model if api_model else None,
                base_url=api_base_url if api_base_url else None
            )
            
            # ç›´æ¥è°ƒç”¨è„šæœ¬æ–¹æ³•å¹¶ä»ä¿å­˜çš„æ–‡ä»¶è¯»å–ç»“æœ
            import tempfile
            import json
            import uuid
            
            # åˆ›å»ºä¸´æ—¶Excelæ–‡ä»¶
            with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as temp_excel:
                temp_excel.write(file_content)
                temp_excel_path = temp_excel.name
            
            # åˆ›å»ºä¸´æ—¶JSONè¾“å‡ºæ–‡ä»¶
            import tempfile
            temp_json_fd, temp_json_path = tempfile.mkstemp(suffix='.json', prefix='web_analysis_')
            os.close(temp_json_fd)  # å…³é—­æ–‡ä»¶æè¿°ç¬¦ï¼Œè®©è„šæœ¬å¯ä»¥å†™å…¥æ–‡ä»¶
            
            try:
                print("ğŸ”„ å¼€å§‹è°ƒç”¨fulltextåˆ†æè„šæœ¬...")
                
                # æ ¹æ®åˆ†ææ¨¡å¼è°ƒç”¨è„šæœ¬æ–¹æ³•
                if analysis_mode == 'head' and num_samples:
                    print(f"ğŸ“Š åˆ†ææ¨¡å¼ï¼šå‰{num_samples}æ¡")
                    script_results = await citation_analyzer.batch_analyze_concurrent(
                        temp_excel_path, num_samples=num_samples
                    )
                elif analysis_mode == 'specific' and specific_rank:
                    print(f"ğŸ“Š åˆ†ææ¨¡å¼ï¼šç¬¬{specific_rank}æ¡")
                    script_results = await citation_analyzer.batch_analyze_concurrent(
                        temp_excel_path, specific_rank=specific_rank
                    )
                elif analysis_mode == 'range' and start_from:
                    print(f"ğŸ“Š åˆ†ææ¨¡å¼ï¼šä»ç¬¬{start_from}æ¡å¼€å§‹ï¼Œ{num_samples or 'å…¨éƒ¨'}æ¡")
                    script_results = await citation_analyzer.batch_analyze_concurrent(
                        temp_excel_path, start_from=start_from, num_samples=num_samples
                    )
                else:
                    # é»˜è®¤åˆ†ææ‰€æœ‰æ•°æ®
                    print("ğŸ“Š åˆ†ææ¨¡å¼ï¼šå…¨éƒ¨æ•°æ®")
                    script_results = await citation_analyzer.batch_analyze_concurrent(temp_excel_path)
                
                print(f"âœ… è„šæœ¬åˆ†æå®Œæˆï¼Œè·å¾—{len(script_results)}æ¡ç»“æœ")
                
                # ä½¿ç”¨è„šæœ¬çš„ä¿å­˜æ–¹æ³•ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                print("ğŸ’¾ ä¿å­˜ä¸´æ—¶ç»“æœæ–‡ä»¶...")
                citation_analyzer.save_results(script_results, temp_json_path)
                
                # åŒæ—¶ä¿å­˜åˆ°é¡¹ç›®çš„data/output/resultsç›®å½•
                import datetime
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                permanent_output_path = os.path.join(
                    os.path.dirname(__file__), "..", "data", "output", "results",
                    f"web_analysis_{analysis_type}_{analysis_mode}_{timestamp}.json"
                )
                
                # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
                print(f"ğŸ“ å‡†å¤‡ä¿å­˜åˆ°ï¼š{permanent_output_path}")
                os.makedirs(os.path.dirname(permanent_output_path), exist_ok=True)
                
                # ä¿å­˜æ°¸ä¹…å‰¯æœ¬
                citation_analyzer.save_results(script_results, permanent_output_path)
                print(f"âœ… Webåˆ†æç»“æœå·²ä¿å­˜åˆ°ï¼š{permanent_output_path}")
                
                # ä»ä¿å­˜çš„æ–‡ä»¶è¯»å–å¹²å‡€çš„æ•°æ®
                print("ğŸ“– ä»æ–‡ä»¶è¯»å–ç»“æœ...")
                with open(temp_json_path, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                
                print(f"âœ… æˆåŠŸè¯»å–{len(results)}æ¡ç»“æœ")
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    os.unlink(temp_json_path)
                    print("ğŸ§¹ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†")
                except:
                    pass
                    
            except Exception as e:
                print(f"âŒ Fulltextåˆ†æè¿‡ç¨‹å‡ºé”™ï¼š{str(e)}")
                import traceback
                traceback.print_exc()
                raise ValueError(f"Fulltextåˆ†ææ‰§è¡Œå¤±è´¥ï¼š{str(e)}")
            finally:
                # æ¸…ç†ä¸´æ—¶Excelæ–‡ä»¶
                try:
                    os.unlink(temp_excel_path)
                except:
                    pass
        
        # æ·»åŠ åˆ†æé€‰é¡¹ä¿¡æ¯åˆ°ç»“æœä¸­
        analysis_info = {
            'analysis_type': analysis_type,
            'analysis_mode': analysis_mode
        }
        if num_samples:
            analysis_info['num_samples'] = num_samples
        if specific_rank:
            analysis_info['specific_rank'] = specific_rank
        if start_from:
            analysis_info['start_from'] = start_from
        
        # ç»Ÿè®¡ç»“æœ
        total_count = len(results)
        success_count = sum(1 for r in results if r.get('api_success', False))
        failed_count = total_count - success_count
        
        # æ¸…ç†è¿”å›ç»“æœä¸­çš„æ— æ•ˆæµ®ç‚¹æ•°
        def clean_for_json(obj):
            """é€’å½’æ¸…ç†å¯¹è±¡ä¸­çš„æ— æ•ˆæµ®ç‚¹æ•°"""
            import math
            import pandas as pd
            import numpy as np
            
            if isinstance(obj, dict):
                return {key: clean_for_json(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [clean_for_json(item) for item in obj]
            elif isinstance(obj, (float, np.floating)):
                if math.isnan(obj) or math.isinf(obj):
                    return None
                return float(obj)
            elif isinstance(obj, (np.integer, np.int64)):
                return int(obj)
            elif hasattr(obj, 'item'):  # numpyæ ‡é‡
                return clean_for_json(obj.item())
            else:
                # ä½¿ç”¨pandasæ£€æŸ¥NaN/NA
                try:
                    if pd.isna(obj):
                        return None
                except (TypeError, ValueError):
                    pass
                return obj
        
        # æ¸…ç†å“åº”æ•°æ®
        response_data = {
            "filename": file.filename,
            "analysis_type": analysis_type,
            "analysis_mode": analysis_mode,
            "total_rows": total_count,
            "success_count": success_count,
            "failed_count": failed_count,
            "results": results[:10] if len(results) > 10 else results,
            "full_results_available": len(results) > 10,
            "output_file_saved": permanent_output_path if 'permanent_output_path' in locals() else None
        }
        
        cleaned_response = clean_for_json(response_data)
        return JSONResponse(content=cleaned_response)
        
    except ValueError as e:
        return JSONResponse(
            status_code=400,
            content={"error": str(e)}
        )
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"}
        )

@app.post("/api/analyze-internal-consistency")
async def analyze_internal_consistency(
    request: Request,
    file: UploadFile = File(...)
):
    """
    ä¸Šä¼ xlsxæ–‡ä»¶è¿›è¡Œå†…éƒ¨ä¸€è‡´æ€§æ£€æµ‹ï¼ˆæ–°ç‰ˆå¹»è§‰æ£€æµ‹ï¼‰
    ä¸ä¾èµ–å¼•æ–‡ï¼Œæ£€æµ‹ç­”æ¡ˆè‡ªèº«çš„é€»è¾‘ä¸€è‡´æ€§
    """
    print("ğŸ” å†…éƒ¨ä¸€è‡´æ€§æ£€æµ‹APIè¢«è°ƒç”¨ï¼")
    
    # è·å–APIé…ç½®
    api_key = request.headers.get('X-API-Key')
    api_provider = request.headers.get('X-API-Provider', 'deepseek')
    api_model = request.headers.get('X-API-Model', '')
    api_base_url = request.headers.get('X-API-Base-URL', '')
    
    print(f"ğŸ”‘ APIé…ç½®: å¯†é’¥={'å·²è®¾ç½®' if api_key else 'æœªè®¾ç½®'}, æä¾›å•†={api_provider}, æ¨¡å‹={api_model or 'é»˜è®¤'}")
    
    if not api_key:
        return JSONResponse(
            status_code=400,
            content={"error": "ç¼ºå°‘API Key"}
        )
    
    # æ£€æŸ¥æ–‡ä»¶ç±»å‹
    if not file.filename.endswith('.xlsx'):
        return JSONResponse(
            status_code=400,
            content={"error": "åªæ”¯æŒxlsxæ ¼å¼æ–‡ä»¶"}
        )
    
    try:
        print("ğŸ” å¼€å§‹å†…éƒ¨ä¸€è‡´æ€§æ£€æµ‹...")
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        file_content = await file.read()
        print(f"ğŸ“ æ–‡ä»¶è¯»å–å®Œæˆï¼Œå¤§å°ï¼š{len(file_content)} bytes")
        
        # è·å–åˆ†æé€‰é¡¹
        analysis_mode = request.headers.get('X-Analysis-Mode', 'all')
        num_samples = request.headers.get('X-Num-Samples')
        specific_rank = request.headers.get('X-Specific-Rank')
        start_from = request.headers.get('X-Start-From')
        concurrent_limit = request.headers.get('X-Concurrent-Limit', '10')
        
        print(f"âš™ï¸ åˆ†æé€‰é¡¹ï¼šmode={analysis_mode}, samples={num_samples}, rank={specific_rank}, start={start_from}, concurrent={concurrent_limit}")
        
        # è½¬æ¢æ•°å€¼å‚æ•°
        try:
            if num_samples:
                num_samples = int(num_samples)
            if specific_rank:
                specific_rank = int(specific_rank)
            if start_from:
                start_from = int(start_from)
            concurrent_limit = int(concurrent_limit)
        except ValueError:
            print("âŒ å‚æ•°æ ¼å¼é”™è¯¯")
            return JSONResponse(
                status_code=400,
                content={"error": "åˆ†æå‚æ•°æ ¼å¼é”™è¯¯"}
            )
        
        # åˆ›å»ºå†…éƒ¨ä¸€è‡´æ€§æ£€æµ‹å™¨
        print(f"ğŸ”„ åˆå§‹åŒ–å†…éƒ¨ä¸€è‡´æ€§æ£€æµ‹å™¨...")
        detector = InternalConsistencyDetector(
            provider=api_provider,
            api_key=api_key,
            base_url=api_base_url if api_base_url else None,
            model=api_model if api_model else None,
            concurrent_limit=concurrent_limit
        )
        
        # æ‰§è¡Œåˆ†æ
        print(f"ğŸ”„ å¼€å§‹å†…éƒ¨ä¸€è‡´æ€§æ£€æµ‹...")
        import datetime
        analysis_time = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        results = await detector.batch_analyze_excel(
            file_content=file_content,
            num_samples=num_samples,
            specific_rank=specific_rank,
            start_from=start_from
        )
        
        print(f"âœ… å†…éƒ¨ä¸€è‡´æ€§æ£€æµ‹å®Œæˆï¼Œè·å¾—{len(results)}æ¡ç»“æœ")
        
        # ç”Ÿæˆæ‘˜è¦
        summary = detector.generate_summary(results)
        print(f"ğŸ“Š åˆ†ææ‘˜è¦ï¼šæˆåŠŸ{summary['success_count']}æ¡ï¼Œé—®é¢˜{summary['problem_count']}æ¡")
        
        # ä¿å­˜ç»“æœåˆ°é¡¹ç›®ç›®å½•
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = os.path.join(
            os.path.dirname(__file__), "..", "data", "output", "results",
            f"internal_consistency_{analysis_mode}_{timestamp}.json"
        )
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        detector.save_results(results, output_path)
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°ï¼š{output_path}")
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "filename": file.filename,
            "analysis_type": "internal_consistency",
            "analysis_mode": analysis_mode,
            "provider": api_provider,
            "model": api_model or "default",
            "analysis_time": analysis_time,
            "total_count": summary['total_count'],
            "success_count": summary['success_count'],
            "failed_count": summary['failed_count'],
            "problem_count": summary['problem_count'],
            "no_problem_count": summary['no_problem_count'],
            "problem_rate": round(summary['problem_rate'], 3),
            "status_distribution": summary['status_distribution'],
            "analysis_summary": summary['analysis_summary'],
            "results": results[:10] if len(results) > 10 else results,  # åªè¿”å›å‰10æ¡è¯¦ç»†ç»“æœ
            "full_results_available": len(results) > 10,
            "output_file_saved": output_path,
            "message": f"å†…éƒ¨ä¸€è‡´æ€§æ£€æµ‹å®Œæˆï¼Œå‘ç°{summary['problem_count']}ä¸ªé—®é¢˜"
        }
        
        return JSONResponse(content=response_data)
        
    except ValueError as e:
        print(f"âŒ å‚æ•°é”™è¯¯ï¼š{str(e)}")
        return JSONResponse(
            status_code=400,
            content={"error": str(e)}
        )
    except Exception as e:
        print(f"âŒ å†…éƒ¨ä¸€è‡´æ€§æ£€æµ‹å¤±è´¥ï¼š{str(e)}")
        import traceback
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={"error": f"å†…éƒ¨ä¸€è‡´æ€§æ£€æµ‹å¤±è´¥ï¼š{str(e)}"}
        )
